# Domain specific sentence tokenizing for German legal decisions

this is modification of the nltk sentence tokenizer to overcome the 
sentence segmentation problems by the vast amount of abbreviations
found in german law texts.



## References

* (nltk documentation)[http://www.nltk.org/_modules/nltk/tokenize/punkt.html]

* (how-to-tweak-the-nltk-sentence-tokenizer)[http://stackoverflow.com/questions/14095971/how-to-tweak-the-nltk-sentence-tokenizer]


